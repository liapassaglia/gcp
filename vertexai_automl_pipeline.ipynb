{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f0899973-4961-42ee-8fd1-5729bd8a0fad",
   "metadata": {},
   "source": [
    "General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "023fca9b-1264-471b-b66a-fde7baee2b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uqq kfp\n",
    "!pip install -Uqq google-cloud-aiplatform\n",
    "!pip install -Uqq google-cloud-pipeline-components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb41afeb-623a-455b-bf87-ab05ed8bb37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from kfp.v2.dsl import component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58b629d3-2120-4dea-8f22-c37a5e7df777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lias-project'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title GCS\n",
    "PROJECT_ID = 'lias-project'\n",
    "PIPELINE_ROOT_PATH = 'gs://vertexai-demo-pipeline'  #@param {type:\"string\"}\n",
    "PIPELINE_NAME = 'cifar10-pipeline-automl' #@param {type:\"string\"}\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3702ba0b-64be-41c3-82f5-44e4edc4cee2",
   "metadata": {},
   "source": [
    "Custom Components for retraining "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "970fb400-8fe4-4180-9c19-e5b2fa9babe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import Artifact, Output\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\"google-cloud-aiplatform\", \"google-cloud-pipeline-components\"]\n",
    ")\n",
    "#to get dataset ID if one exists\n",
    "def get_dataset_id(project_id: str, \n",
    "                  location: str,\n",
    "                  dataset_name: str,\n",
    "                  dataset_path: str,\n",
    "                  dataset: Output[Artifact]) -> str:\n",
    "    from google.cloud import aiplatform\n",
    "    from google.cloud.aiplatform.datasets.image_dataset import ImageDataset\n",
    "    from google_cloud_pipeline_components.types.artifact_types import VertexDataset\n",
    "\n",
    "    \n",
    "    aiplatform.init(project=project_id, location=location)\n",
    "    \n",
    "    datasets = aiplatform.ImageDataset.list(project=project_id,\n",
    "                                            location=location,\n",
    "                                            filter=f'display_name={dataset_name}')\n",
    "    \n",
    "    if len(datasets) > 0:\n",
    "        dataset.metadata['resourceName'] = f'projects/{project_id}/locations/{location}/datasets/{datasets[0].name}'\n",
    "        return f'projects/{project_id}/locations/{location}/datasets/{datasets[0].name}'\n",
    "    else:\n",
    "        return 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9af1f804-2abb-436f-8d6e-eec5e905c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import Artifact, Output\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\"google-cloud-aiplatform\", \"google-cloud-pipeline-components\"]\n",
    ")\n",
    "#to get model ID if one exists\n",
    "def get_model_id(project_id: str, \n",
    "                 location: str,\n",
    "                 model_name: str,\n",
    "                 model: Output[Artifact]) -> str:\n",
    "    from google.cloud import aiplatform\n",
    "    from google_cloud_pipeline_components.types.artifact_types import VertexModel\n",
    "    \n",
    "    aiplatform.init(project=project_id, location=location)\n",
    "    \n",
    "    models = aiplatform.Model.list(project=project_id,\n",
    "                                   location=location,\n",
    "                                   filter=f'display_name={model_name}')\n",
    "    \n",
    "    if len(models) > 0:\n",
    "        model.metadata['resourceName'] = f'projects/{project_id}/locations/{location}/models/{models[0].name}'\n",
    "        return f'projects/{project_id}/locations/{location}/models/{models[0].name}'\n",
    "    else:\n",
    "        return 'None'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "65383b78-6793-4cad-be00-2a263378a774",
   "metadata": {},
   "source": [
    "Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "052ceeb8-902b-4e26-a3d2-ff7e1c10b6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform.datasets.image_dataset import ImageDataset\n",
    "from google_cloud_pipeline_components.types.artifact_types import VertexDataset\n",
    "\n",
    "#define the workflow of the pipeline\n",
    "@kfp.dsl.pipeline(\n",
    "    name=PIPELINE_NAME,\n",
    "    pipeline_root=PIPELINE_ROOT_PATH)\n",
    "def pipeline(project_id: str, \n",
    "             location: str,\n",
    "             dataset_name: str,\n",
    "             dataset_path: str,\n",
    "             base_model_name: str):\n",
    "    \n",
    "    #the first step of the workflow is a dataset generator.\n",
    "    #this step checks if the dataset already exists, if no dataset exists, it will create one\n",
    "    dataset_op = get_dataset_id(project_id=project_id,\n",
    "                                location=location,\n",
    "                                dataset_name=dataset_name,\n",
    "                                dataset_path=dataset_path)\n",
    "    \n",
    "    #it takes a Google Cloud pipeline componenet, provides the necessary input arguments, and uses ds_op to define its output.\n",
    "    #ds_op only stores the definition of the output, not the actual returned object from the execution\n",
    "    with kfp.dsl.Condition(dataset_op.outputs['Output'] == 'None', name=\"create dataset\"):\n",
    "        ds_op = gcc_aip.ImageDatasetCreateOp(\n",
    "            project=project_id,\n",
    "            display_name=dataset_name,\n",
    "            gcs_source=dataset_path,\n",
    "            import_schema_uri=aiplatform.schema.dataset.ioformat.image.single_label_classification,\n",
    "        )\n",
    "        ds_op.after(dataset_op)\n",
    "        \n",
    "        #the second step is a model training component. \n",
    "        #it takes the dataset outputted from the first step and supplies it as an input argument to the component\n",
    "        #it puts the outputs into training_job_run_op\n",
    "        training_job_run_op = gcc_aip.AutoMLImageTrainingJobRunOp(\n",
    "            project=project_id,\n",
    "            display_name=\"train-cifar10-automl\",\n",
    "            prediction_type=\"classification\",\n",
    "            model_type=\"CLOUD\",\n",
    "            dataset=ds_op.outputs[\"dataset\"],\n",
    "            model_display_name=\"cifar10-model\",\n",
    "            training_fraction_split=0.6,\n",
    "            validation_fraction_split=0.2,\n",
    "            test_fraction_split=0.2,\n",
    "            budget_milli_node_hours=8000,\n",
    "        )\n",
    "        training_job_run_op.after(ds_op)\n",
    "\n",
    "        #the third and fourth step are for deploying the model\n",
    "        create_endpoint_op = gcc_aip.EndpointCreateOp(\n",
    "            project=project_id,\n",
    "            display_name = \"cifar10-automl-endpoint\",\n",
    "        )\n",
    "        create_endpoint_op.after(training_job_run_op)\n",
    "\n",
    "        model_deploy_op = gcc_aip.ModelDeployOp(\n",
    "            model=training_job_run_op.outputs[\"model\"],\n",
    "            endpoint=create_endpoint_op.outputs['endpoint'],\n",
    "            automatic_resources_min_replica_count=1,\n",
    "            automatic_resources_max_replica_count=1,\n",
    "        )\n",
    "        model_deploy_op.after(create_endpoint_op) \n",
    "\n",
    "    with kfp.dsl.Condition(dataset_op.outputs['Output'] != 'None', name=\"update dataset\"):\n",
    "        ds_op = gcc_aip.ImageDatasetImportDataOp(\n",
    "            project=project_id,\n",
    "            dataset=dataset_op.outputs['dataset'],\n",
    "            gcs_source=dataset_path,\n",
    "            import_schema_uri=aiplatform.schema.dataset.ioformat.image.single_label_classification\n",
    "        )\n",
    "        ds_op.after(dataset_op)\n",
    "\n",
    "        model_op = get_model_id(\n",
    "            project_id=project_id,\n",
    "            location=location,\n",
    "            model_name=base_model_name\n",
    "        )\n",
    "        model_op.after(ds_op)\n",
    "\n",
    "        with kfp.dsl.Condition(model_op.outputs['Output'] != 'None', name='model exist'):\n",
    "            training_job_run_op = gcc_aip.AutoMLImageTrainingJobRunOp(\n",
    "                  project=project_id,\n",
    "                  display_name=\"train-cifar10-automl\",\n",
    "                  prediction_type=\"classification\",\n",
    "                  model_type=\"CLOUD\",\n",
    "                  base_model=model_op.outputs['model'],\n",
    "                  dataset=ds_op.outputs[\"dataset\"],\n",
    "                  model_display_name=\"cifar10-model\",\n",
    "                  training_fraction_split=0.6,\n",
    "                  validation_fraction_split=0.2,\n",
    "                  test_fraction_split=0.2,\n",
    "                  budget_milli_node_hours=8000,\n",
    "            )\n",
    "            training_job_run_op.after(model_op)\n",
    "\n",
    "            create_endpoint_op = gcc_aip.EndpointCreateOp(\n",
    "                project=project_id,\n",
    "                display_name = \"cifar10-automl-endpoint\",\n",
    "            )\n",
    "            create_endpoint_op.after(training_job_run_op)\n",
    "\n",
    "            model_deploy_op = gcc_aip.ModelDeployOp(\n",
    "              model=training_job_run_op.outputs[\"model\"],\n",
    "              endpoint=create_endpoint_op.outputs['endpoint'],\n",
    "              automatic_resources_min_replica_count=1,\n",
    "              automatic_resources_max_replica_count=1,\n",
    "              traffic_split={\"0\": 100},\n",
    "            )\n",
    "            model_deploy_op.after(create_endpoint_op)      \n",
    "\n",
    "        with kfp.dsl.Condition(model_op.outputs['Output'] == 'None', name='model not exist'):\n",
    "            training_job_run_op = gcc_aip.AutoMLImageTrainingJobRunOp(\n",
    "              project=project_id,\n",
    "              display_name=\"train-cifar10-automl\",\n",
    "              prediction_type=\"classification\",\n",
    "              model_type=\"CLOUD\",\n",
    "              dataset=ds_op.outputs[\"dataset\"],\n",
    "              model_display_name=\"cifar10-model\",\n",
    "              training_fraction_split=0.6,\n",
    "              validation_fraction_split=0.2,\n",
    "              test_fraction_split=0.2,\n",
    "              budget_milli_node_hours=8000,\n",
    "            )\n",
    "            training_job_run_op.after(model_op)\n",
    "\n",
    "            create_endpoint_op = gcc_aip.EndpointCreateOp(\n",
    "              project=project_id,\n",
    "              display_name = \"cifar10-automl-endpoint\",\n",
    "            )\n",
    "            create_endpoint_op.after(training_job_run_op)\n",
    "\n",
    "            model_deploy_op = gcc_aip.ModelDeployOp(\n",
    "              model=training_job_run_op.outputs[\"model\"],\n",
    "              endpoint=create_endpoint_op.outputs['endpoint'],\n",
    "              automatic_resources_min_replica_count=1,\n",
    "              automatic_resources_max_replica_count=1,\n",
    "            )\n",
    "            model_deploy_op.after(create_endpoint_op) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "f3edf6db-f819-40b0-b9c9-329241987264",
   "metadata": {},
   "source": [
    "Compile pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd1f37a9-d770-48ba-ac70-6b27eee8a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_spec_file = 'cifar10_classification_pipeline.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "076ebf8e-9d81-4254-befa-0a06c4400927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "        pipeline_func=pipeline,\n",
    "        package_path=pipeline_spec_file)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5bf14b21-d10a-4226-9bca-70dc4865741f",
   "metadata": {},
   "source": [
    "Create GCP bucket & copy the pipeline spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4520851b-b0e1-4660-a22d-f726edd78205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://vertexai-demo-pipeline/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'vertexai-demo-pipeline' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n",
      "Copying file://cifar10_classification_pipeline.json [Content-Type=application/json]...\n",
      "/ [1 files][ 84.0 KiB/ 84.0 KiB]                                                \n",
      "Operation completed over 1 objects/84.0 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "#@title GCS\n",
    "REGION = \"us-central1\" #@param {type:\"string\"}\n",
    "\n",
    "!gsutil mb -l {REGION} {PIPELINE_ROOT_PATH}\n",
    "!gsutil cp {pipeline_spec_file} {PIPELINE_ROOT_PATH}/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c92ea74f-ee0d-440f-bffa-c1370ea0b557",
   "metadata": {},
   "source": [
    "Test pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2feb6696-1530-4bf1-87d1-4b06d37025ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_CLOUD_PROJECT'] = PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d761dd50-bb90-496a-85f3-e15e6d6807a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/425772488260/locations/us-central1/pipelineJobs/cifar10-pipeline-automl-20221205175208\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/425772488260/locations/us-central1/pipelineJobs/cifar10-pipeline-automl-20221205175208')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/cifar10-pipeline-automl-20221205175208?project=425772488260\n"
     ]
    }
   ],
   "source": [
    "location = 'us-central1'\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"automl-image-training-v2\",\n",
    "    template_path=\"cifar10_classification_pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT_PATH,\n",
    "    parameter_values={\n",
    "        'project_id': PROJECT_ID,\n",
    "        'location': REGION,\n",
    "        'dataset_name': 'my-cifar10-dataset-1012',\n",
    "        'dataset_path': 'gs://demo-cifar10-dataset-annotations-1012/span-1/annotations.csv',\n",
    "        'base_model_name': 'cifar10-model',\n",
    "    }\n",
    ")\n",
    "\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af69952-415c-43ca-9496-9cb51be500e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-3.m100",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
